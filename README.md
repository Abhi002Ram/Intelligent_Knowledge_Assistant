# Intelligent Knowledge Assistant using Retrieval-Augmented Generation (RAG)

## üìå Overview
The Intelligent Knowledge Assistant is a fully local, open-source Retrieval-Augmented Generation (RAG) system that answers questions from domain-specific documents with strong factual grounding and explicit source citations.

The system combines semantic retrieval with local large language model inference to ensure that generated answers are based strictly on retrieved evidence, minimizing hallucinations and unsupported claims.


## üß© Models Used

### Embedding Model
- **Name**: `intfloat/e5-base-v2`
- **Purpose**: Semantic retrieval
- **Characteristics**:
  - Strong retrieval performance
  - Open-source
  - Efficient CPU inference

### Language Model (LLM)
- **Name**: Mistral 7B Instruct
- **Format**: GGUF
- **Quantization**: Q4_K_M
- **Runtime**: `llama.cpp` via `llama-cpp-python`
- **Characteristics**:
  - Instruction-tuned
  - Optimized for local inference
  - No external dependencies


## Tech Stack
- Programming Language: Python 3.10+
- Embeddings: Sentence Transformers
- Vector Database: FAISS
- LLM Runtime: llama.cpp (llama-cpp-python)
- Configuration: YAML
- UI: Streamlit
- Persistence: NumPy (.npy), FAISS index serialization


## üèóÔ∏è Architecture
The system follows a modular, pipeline-based architecture:
- **Document ingestion and cleaning**  
  Loads raw domain documents and performs basic normalization.

- **Recursive text chunking**  
  Splits documents into semantically meaningful chunks suitable for retrieval.

- **Embeddings generation**  
  Uses `intfloat/e5-base-v2` to convert text chunks and queries into dense vectors.

- **Vector search (FAISS)**  
  Performs efficient similarity search to retrieve the top-K relevant chunks.

- **Local LLM inference**  
  Uses Mistral 7B Instruct (GGUF format) via `llama.cpp` for grounded answer generation.

- **Streamlit UI**  
  Provides an interactive interface for querying the knowledge base and viewing sources.


## üéØ Application Flow

    User Question
    ‚Üì
    Query Embedding (e5-base-v2)
    ‚Üì
    FAISS Vector Search
    ‚Üì
    Top-K Relevant Chunks
    ‚Üì
    Grounded Prompt Construction
    ‚Üì
    Local LLM Inference (Mistral 7B)
    ‚Üì
    Answer + Source Citations



## üéØ Design Decisions
- **No fine-tuning**  
  The system intentionally avoids fine-tuning to test and enforce hallucination control purely through retrieval grounding.

- **Strict separation of retrieval and generation**  
  Retrieval determines *what information is available*; generation only summarizes retrieved content.

- **Local inference**  
  All models run locally to ensure privacy, cost control, and independence from external APIs.

- **Configuration-driven system**  
  Model, embedding, and retrieval parameters are externalized in YAML configuration files.

- **Persistent embeddings and indexes**  
  Embeddings and FAISS indexes are stored on disk to avoid recomputation and improve startup performance.

---

## üìÇ Project Structure

        intelligent-knowledge-assistant/
        ‚îÇ
        ‚îú‚îÄ‚îÄ data/
        ‚îÇ   ‚îî‚îÄ‚îÄ raw/                  # Domain documents (product, policy, FAQ, technical)
        ‚îÇ
        ‚îú‚îÄ‚îÄ ingestion/                # Document loading & cleaning
        ‚îú‚îÄ‚îÄ chunking/                 # Recursive text chunking
        ‚îú‚îÄ‚îÄ embeddings/               # Embedding model wrapper
        ‚îú‚îÄ‚îÄ vectorstore/              # FAISS index logic
        ‚îú‚îÄ‚îÄ retrieval/                # Top-K retrieval logic
        ‚îú‚îÄ‚îÄ llm/                      # Local LLM wrapper (llama.cpp)
        ‚îú‚îÄ‚îÄ rag/                      # RAG orchestration
        ‚îÇ
        ‚îú‚îÄ‚îÄ configs/
        ‚îÇ   ‚îú‚îÄ‚îÄ model.yaml            # All runtime configuration
        ‚îÇ   ‚îî‚îÄ‚îÄ config_loader.py
        ‚îÇ
        ‚îú‚îÄ‚îÄ evaluation/
        ‚îÇ   ‚îú‚îÄ‚îÄ eval_questions.json   # Evaluation question set
        ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py
        ‚îÇ
        ‚îú‚îÄ‚îÄ ui/
        ‚îÇ   ‚îî‚îÄ‚îÄ app.py                # Streamlit UI
        ‚îÇ
        ‚îú‚îÄ‚îÄ artifacts/                # Auto-generated (ignored in Git)
        ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embeddings.npy
        ‚îÇ   ‚îî‚îÄ‚îÄ faiss/
        ‚îÇ       ‚îî‚îÄ‚îÄ index.faiss
        ‚îÇ
        ‚îú‚îÄ‚îÄ main.py                   # CLI / non-UI entry point
        ‚îú‚îÄ‚îÄ requirements.txt
        ‚îú‚îÄ‚îÄ README.md


## üöÄ Running the System

- Install Dependencies
   > pip install -r requirements.txt

- **Run with Streamlit UI**
   >streamlit run ui/app.py

 - Interactive question answering
 - Displays grounded answers and source files
 - Pipeline is cached per session for performance

- **Run via CLI**
  >python main.py

 - Useful for testing, batch execution, or non-UI usage
 - Uses the same configuration and pipeline as the UI


## üíæ Artifacts and Persistence

 - The system persists heavy computations in the artifacts/ directory:

    artifacts/
    ‚îú‚îÄ‚îÄ embeddings/embeddings.npy
    ‚îî‚îÄ‚îÄ faiss/index.faiss

 - Created automatically on first run
 - Reused on subsequent runs
 - Not committed to version control
 - Rebuilt only when source documents change


## üß™ Evaluation

 - Evaluation is performed using predefined question sets located in:
    evaluation/eval_questions.json

 - The evaluation process focuses on:
    - Answer grounding
    - Hallucination avoidance
    - Correct refusal behavior
    - Source citation correctness

 - Evaluation is currently performed through controlled queries and manual inspection.
 - Evaluation is run explicitly via a separate script and is not executed as part of the UI or main runtime.


## üõ°Ô∏è Hallucination Control

The system enforces strict grounding by:
- Passing only retrieved chunks to the LLM
- Explicitly instructing the model to refuse when information is absent
- Using low-temperature generation
- Avoiding fine-tuning entirely

If evidence is missing, the system responds with:
> ‚ÄúThe information is not available in the provided documents.‚Äù



## ‚ö†Ô∏è Limitations

 - Single-user local deployment

 - CPU-based inference only

 - Manual evaluation (no automated metrics)

 - No authentication or access control

 - Designed for local usage and experimentation


## üß† Key Concepts Demonstrated

 - Retrieval-Augmented Generation (RAG)

 - Dense vector embeddings

 - Semantic similarity search

 - Local large language model inference

 - Prompt grounding and citation

 - Configuration-driven ML systems

 - Persistent vector stores

 - Interactive ML applications with Streamlit






